{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing vanilla code with LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:32:26.686708Z",
     "iopub.status.busy": "2025-08-22T21:32:26.686301Z",
     "iopub.status.idle": "2025-08-22T21:32:26.693882Z",
     "shell.execute_reply": "2025-08-22T21:32:26.692836Z",
     "shell.execute_reply.started": "2025-08-22T21:32:26.686667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "agentic-ops\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(load_dotenv('../.env'))\n",
    "print(os.environ['LANGSMITH_PROJECT'])\n",
    "os.environ['LANGSMITH_TRACING']=\"true\"\n",
    "os.environ['USER_AGENT'] = 'myagent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:16:06.640331Z",
     "iopub.status.busy": "2025-08-22T21:16:06.639401Z",
     "iopub.status.idle": "2025-08-22T21:16:12.318642Z",
     "shell.execute_reply": "2025-08-22T21:16:12.317946Z",
     "shell.execute_reply.started": "2025-08-22T21:16:06.640295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'filename': 'perf_train_cpu_many.md'}, page_content='\"\\\\n\\\\n# Efficient Training on Multiple CPUs\\\\n\\\\nWhen training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently.\\\\n\\\\n## Intel\\\\u00ae oneCCL Bindings for PyTorch\\\\n\\\\n[Intel\\\\u00ae oneCCL](https://github.com/oneapi-src/oneCCL) (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the [oneCCL documentation](https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html) and [oneCCL specification](https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html).\\\\n\\\\nModule `oneccl_bindings_for_pytorch` (`torch_ccl` before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now\\\\n\\\\nCheck more detailed information for [oneccl_bind_pt](https://github.com/intel/torch-ccl).\\\\n\\\\n### Intel\\\\u00ae oneCCL Bindings for PyTorch installation:\\\\n\\\\nWheel files are available for the following Python versions:\\\\n\\\\n| Extension Version | Python 3.6 | Python 3.7 | Python 3.8 | Python 3.9 | Python 3.10 |\\\\n\\\\n| :---------------: | :--------: | :--------: | :--------: | :--------: | :---------: |\\\\n\\\\n| 1.13.0            |            | \\\\u221a          | \\\\u221a          | \\\\u221a          | \\\\u221a           |\\\\n\\\\n| 1.12.100          |            | \\\\u221a          | \\\\u221a          | \\\\u221a          | \\\\u221a           |\\\\n\\\\n| 1.12.0            |            | \\\\u221a          | \\\\u221a          | \\\\u221a          | \\\\u221a           |\\\\n\\\\n| 1.11.0            |            |')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def preprocess_dataset(docs_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=700,\n",
    "        chunk_overlap=50,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    return doc_splits\n",
    "# https://huggingface.co/datasets/m-ric/transformers_documentation_en\n",
    "transformers_doc = HuggingFaceDatasetLoader(\"m-ric/transformers_documentation_en\", \"text\")\n",
    "docs = preprocess_dataset(transformers_doc.load()[:50])\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:16:18.017134Z",
     "iopub.status.busy": "2025-08-22T21:16:18.016381Z",
     "iopub.status.idle": "2025-08-22T21:16:41.539503Z",
     "shell.execute_reply": "2025-08-22T21:16:41.538641Z",
     "shell.execute_reply.started": "2025-08-22T21:16:18.017103Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "vectorstore = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    OpenAIEmbeddings(model=os.environ[\"EMBEDDING_MODEL\"]),\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"documentations\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:16:41.541483Z",
     "iopub.status.busy": "2025-08-22T21:16:41.540726Z",
     "iopub.status.idle": "2025-08-22T21:16:42.934714Z",
     "shell.execute_reply": "2025-08-22T21:16:42.933843Z",
     "shell.execute_reply.started": "2025-08-22T21:16:41.541383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'filename': 'philosophy.md', '_id': '4efe0c99d8744e159d41bdded1c0530b', '_collection_name': 'documentations'}, page_content=\"provided by the official authors\\\\n\\\\n    of said architecture.\\\\n\\\\n  - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\\\\n\\\\n    *pytorchic* as it could be as a result of being converted TensorFlow code and vice versa.\\\\n\\\\nA few other goals:\\\\n\\\\n- Expose the models' internals as consistently as possible:\\\\n\\\\n  - We give access, using a single API, to the full hidden-states and attention weights.\\\\n\\\\n  - The preprocessing classes and base model APIs are standardized to easily switch between models.\\\\n\\\\n- Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:\\\\n\\\\n  - A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.\\\\n\\\\n  - Simple ways to mask and prune Transformer heads.\\\\n\\\\n- Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another.\\\\n\\\\n## Main concepts\\\\n\\\\nThe library is built around three types of classes for each model:\\\\n\\\\n- **Model classes** can be PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)), Keras models ([tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)) or JAX/Flax models ([flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)) that work with the pretrained weights provided in the library.\\\\n\\\\n- **Configuration classes** store the hyperparameters required to build a model (such as the number of layers and hidden size). You don't always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).\\\\n\\\\n- **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.\\\\n\\\\nAll these classes can be instantiated from pretrained instances, saved locally, and shared on the Hub with three methods:\\\\n\\\\n- `from_pretrained()` lets you instantiate a model, configuration, and preprocessing class from a pretrained version either\\\\n\\\\n  provided by the library itself (the supported models can be found on the [Model Hub](https://huggingface.co/models))\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is huggingface ? \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:17:40.003959Z",
     "iopub.status.busy": "2025-08-22T21:17:40.003163Z",
     "iopub.status.idle": "2025-08-22T21:17:40.011399Z",
     "shell.execute_reply": "2025-08-22T21:17:40.010389Z",
     "shell.execute_reply.started": "2025-08-22T21:17:40.003925Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = os.environ[\"OPENAI_MODEL\"]\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return OpenAI().chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:18:33.738588Z",
     "iopub.status.busy": "2025-08-22T21:18:33.737853Z",
     "iopub.status.idle": "2025-08-22T21:18:37.963324Z",
     "shell.execute_reply": "2025-08-22T21:18:37.962678Z",
     "shell.execute_reply.started": "2025-08-22T21:18:33.738556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is used for providing easy access to state-of-the-art machine learning models, especially in natural language processing (NLP), computer vision, and audio tasks. It offers tools and libraries to download, fine-tune, and deploy pretrained models for tasks like translation, text generation, summarization, and more. The platform also enables sharing and collaboration on models through the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is huggingface used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"sample_runtime\": \"metadata added\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See tracing UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:21:15.247046Z",
     "iopub.status.busy": "2025-08-22T21:21:15.246741Z",
     "iopub.status.idle": "2025-08-22T21:21:15.253543Z",
     "shell.execute_reply": "2025-08-22T21:21:15.252626Z",
     "shell.execute_reply.started": "2025-08-22T21:21:15.247024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@traceable(metadata={\"vectordb\": \"qdrant\"})\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER})\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return OpenAI().chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:21:23.064567Z",
     "iopub.status.busy": "2025-08-22T21:21:23.063772Z",
     "iopub.status.idle": "2025-08-22T21:21:26.070113Z",
     "shell.execute_reply": "2025-08-22T21:21:26.069160Z",
     "shell.execute_reply.started": "2025-08-22T21:21:23.064534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is used for providing easy access to state-of-the-art machine learning models, especially in natural language processing (NLP) tasks like translation, text generation, and language modeling. It offers a unified library (Transformers) that allows users to download, fine-tune, and deploy pretrained models for various tasks using simple APIs. The platform also supports sharing and collaborating on models through the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is huggingface used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"sample_runtime\": \"metadata added\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:24:12.189649Z",
     "iopub.status.busy": "2025-08-22T21:24:12.189146Z",
     "iopub.status.idle": "2025-08-22T21:24:12.193491Z",
     "shell.execute_reply": "2025-08-22T21:24:12.192615Z",
     "shell.execute_reply.started": "2025-08-22T21:24:12.189618Z"
    }
   },
   "source": [
    "# Use run_type to recognize as a distinct category in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:23:18.922110Z",
     "iopub.status.busy": "2025-08-22T21:23:18.921596Z",
     "iopub.status.idle": "2025-08-22T21:23:18.929818Z",
     "shell.execute_reply": "2025-08-22T21:23:18.928764Z",
     "shell.execute_reply.started": "2025-08-22T21:23:18.922085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'message': {'role': 'assistant',\n",
       "    'content': 'Sure, what time would you like to book the table for?'}}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"Sure, what time would you like to book the table for?\"\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "@traceable(run_type=\"llm\") \n",
    "def chat_model(messages: list):\n",
    "  return output\n",
    "\n",
    "chat_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:33:24.835150Z",
     "iopub.status.busy": "2025-08-22T21:33:24.834824Z",
     "iopub.status.idle": "2025-08-22T21:33:24.839230Z",
     "shell.execute_reply": "2025-08-22T21:33:24.838420Z",
     "shell.execute_reply.started": "2025-08-22T21:33:24.835127Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:34:09.803695Z",
     "iopub.status.busy": "2025-08-22T21:34:09.803387Z",
     "iopub.status.idle": "2025-08-22T21:34:09.824164Z",
     "shell.execute_reply": "2025-08-22T21:34:09.823346Z",
     "shell.execute_reply.started": "2025-08-22T21:34:09.803672Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running twice to show 2 turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:34:15.539985Z",
     "iopub.status.busy": "2025-08-22T21:34:15.539338Z",
     "iopub.status.idle": "2025-08-22T21:34:20.346848Z",
     "shell.execute_reply": "2025-08-22T21:34:20.345896Z",
     "shell.execute_reply.started": "2025-08-22T21:34:15.539952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is a company and open-source platform that provides state-of-the-art machine learning models and tools, especially for natural language processing, computer vision, audio, and multimodal tasks. Their Transformers library supports easy downloading, training, and sharing of pretrained models across frameworks like PyTorch, TensorFlow, and JAX. The Hugging Face Model Hub allows users to share, version, and collaborate on models with a GitHub-like interface.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is huggingface ?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T21:34:22.891952Z",
     "iopub.status.busy": "2025-08-22T21:34:22.891381Z",
     "iopub.status.idle": "2025-08-22T21:34:27.974173Z",
     "shell.execute_reply": "2025-08-22T21:34:27.973185Z",
     "shell.execute_reply.started": "2025-08-22T21:34:22.891918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Hugging Face, datasets are collections of data (such as text, images, or audio) organized for machine learning tasks, often stored on disk to efficiently handle large sizes. They can be easily loaded, processed, and transformed using the ðŸ¤— Datasets library, which provides tools for mapping preprocessing functions, batching, and streaming data without inflating memory usage. These datasets are commonly used for training, evaluating, and testing models in NLP, vision, and audio tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"what are datasets in huggingface then ? \"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traces UI tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
