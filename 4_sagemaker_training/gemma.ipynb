{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma on Sagemaker with QLoRA\n",
    "\n",
    "- Notebook running with a different EC2 instance type\n",
    "- Use Huggingface library\n",
    "  - SQL generator Dataset from Huggingface\n",
    "  - Transformer with Gemma model\n",
    "  - SFT Trainer from TRL library by HugginFace\n",
    "  - QLoRA based training from PEFT\n",
    "  - Deploy using saved artifacts on S3\n",
    "  - evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:31:01.350382Z",
     "iopub.status.busy": "2025-08-22T22:31:01.349954Z",
     "iopub.status.idle": "2025-08-22T22:31:03.138757Z",
     "shell.execute_reply": "2025-08-22T22:31:03.138018Z",
     "shell.execute_reply.started": "2025-08-22T22:31:01.350356Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.1 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: sagemaker==2.227.0 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 2)) (2.227.0)\n",
      "Requirement already satisfied: boto3==1.40.16 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 3)) (1.40.16)\n",
      "Requirement already satisfied: botocore==1.40.16 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 4)) (1.40.16)\n",
      "Requirement already satisfied: s3transfer==0.13.1 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.12/site-packages (from -r ./requirements_local.txt (line 6)) (2.18.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.25.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (6.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.7.7)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.23.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.3.8)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.26.19)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (7.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3==1.40.16->-r ./requirements_local.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore==1.40.16->-r ./requirements_local.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (3.12.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.12/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 6)) (0.33.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.16->-r ./requirements_local.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.1.2->aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0->-r ./requirements_local.txt (line 6)) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2025.6.15)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.7.7)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.3.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./requirements_local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure HF_TOKEN is there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:24.593765Z",
     "iopub.status.busy": "2025-08-22T22:32:24.593345Z",
     "iopub.status.idle": "2025-08-22T22:32:24.604782Z",
     "shell.execute_reply": "2025-08-22T22:32:24.603664Z",
     "shell.execute_reply.started": "2025-08-22T22:32:24.593737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Your UID is 7l4srb9\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(load_dotenv('../.env'))\n",
    "\n",
    "if not os.environ['HF_TOKEN']:\n",
    "    raise(\"Please load token\")\n",
    "\n",
    "uid=os.environ[\"UID\"]\n",
    "print(f\"Your UID is {uid}\")\n",
    "\n",
    "job_name = f\"{uid}-qlora-gemma-2b-sql-generator\"\n",
    "deploy_model_name = f\"{uid}-sql-generator-model\"\n",
    "s3_bucket = os.environ['S3_WORKSHOP_BUCKET']\n",
    "train_instance = 'ml.g5.2xlarge'\n",
    "deploy_instance = 'ml.g5.2xlarge'\n",
    "model_id = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:24.911904Z",
     "iopub.status.busy": "2025-08-22T22:32:24.911510Z",
     "iopub.status.idle": "2025-08-22T22:32:25.530550Z",
     "shell.execute_reply": "2025-08-22T22:32:25.529737Z",
     "shell.execute_reply.started": "2025-08-22T22:32:24.911878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sagemaker-dhs-2025-llmops-workshop got response BucketAlreadyOwnedByYou\n",
      "role ='arn:aws:iam::009676737623:role/service-role/AmazonSageMaker-ExecutionRole-20250814T174659'\n",
      "s3_bucket ='sagemaker-dhs-2025-llmops-workshop'\n",
      "sess.boto_region_name ='ap-south-1'\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import sagemaker\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# role_name = \"llmops_workshop_sagemaker_exec_role\"\n",
    "train_local, train_path = './tmp/train.jsonl', f\"{uid}/dataset/train.jsonl\"\n",
    "test_local, test_path = './tmp/test.jsonl', f\"{uid}/dataset/test.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "# s3_bucket = sess.default_bucket()\n",
    "def create_bucket(bucket_name, region=\"ap-south-1\"):\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    try:\n",
    "        location = {'LocationConstraint': region}\n",
    "        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        print(f\"Bucket {bucket_name} got response {e.response['Error']['Code']}\")\n",
    "\n",
    "create_bucket(s3_bucket)\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    # role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    role = \"arn:aws:iam::009676737623:role/llmops_workshop_sagemaker_exec_role\"\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=s3_bucket, default_bucket_prefix=uid)\n",
    "\n",
    "print(f\"{role =}\")\n",
    "print(f\"{s3_bucket =}\")\n",
    "print(f\"{sess.boto_region_name =}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:25.531809Z",
     "iopub.status.busy": "2025-08-22T22:32:25.531588Z",
     "iopub.status.idle": "2025-08-22T22:32:28.508807Z",
     "shell.execute_reply": "2025-08-22T22:32:28.508030Z",
     "shell.execute_reply.started": "2025-08-22T22:32:25.531790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c558b1e0e3a465f9050e390706da438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "DATASET = \"gretelai/synthetic_text_to_sql\"\n",
    "ds = datasets.load_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:28.510567Z",
     "iopub.status.busy": "2025-08-22T22:32:28.509825Z",
     "iopub.status.idle": "2025-08-22T22:32:28.515742Z",
     "shell.execute_reply": "2025-08-22T22:32:28.514777Z",
     "shell.execute_reply.started": "2025-08-22T22:32:28.510541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (100000, 11), 'test': (5851, 11)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:28.517543Z",
     "iopub.status.busy": "2025-08-22T22:32:28.517215Z",
     "iopub.status.idle": "2025-08-22T22:32:28.523030Z",
     "shell.execute_reply": "2025-08-22T22:32:28.522386Z",
     "shell.execute_reply.started": "2025-08-22T22:32:28.517521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int32', id=None),\n",
       " 'domain': Value(dtype='string', id=None),\n",
       " 'domain_description': Value(dtype='string', id=None),\n",
       " 'sql_complexity': Value(dtype='string', id=None),\n",
       " 'sql_complexity_description': Value(dtype='string', id=None),\n",
       " 'sql_task_type': Value(dtype='string', id=None),\n",
       " 'sql_task_type_description': Value(dtype='string', id=None),\n",
       " 'sql_prompt': Value(dtype='string', id=None),\n",
       " 'sql_context': Value(dtype='string', id=None),\n",
       " 'sql': Value(dtype='string', id=None),\n",
       " 'sql_explanation': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:28.524423Z",
     "iopub.status.busy": "2025-08-22T22:32:28.523851Z",
     "iopub.status.idle": "2025-08-22T22:32:28.528798Z",
     "shell.execute_reply": "2025-08-22T22:32:28.528134Z",
     "shell.execute_reply.started": "2025-08-22T22:32:28.524398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 5097\n",
      "\n",
      "\n",
      "domain: forestry\n",
      "\n",
      "\n",
      "domain_description: Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "\n",
      "\n",
      "sql_complexity: single join\n",
      "\n",
      "\n",
      "sql_complexity_description: only one join (specify inner, outer, cross)\n",
      "\n",
      "\n",
      "sql_task_type: analytics and reporting\n",
      "\n",
      "\n",
      "sql_task_type_description: generating reports, dashboards, and analytical insights\n",
      "\n",
      "\n",
      "sql_prompt: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "\n",
      "\n",
      "sql_context: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "\n",
      "\n",
      "sql: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "\n",
      "\n",
      "sql_explanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label, val in ds[\"train\"][0].items():\n",
    "    print(f\"{label}: {val}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:28.530058Z",
     "iopub.status.busy": "2025-08-22T22:32:28.529770Z",
     "iopub.status.idle": "2025-08-22T22:32:28.534425Z",
     "shell.execute_reply": "2025-08-22T22:32:28.533570Z",
     "shell.execute_reply.started": "2025-08-22T22:32:28.530037Z"
    }
   },
   "outputs": [],
   "source": [
    "USER_PROMPT_TEMPLATE = \"\"\" \n",
    "You are a database management system expert, proficient in Structured Query Language (SQL). \n",
    "Your job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \n",
    "Use SQLite syntax and please output only SQL without any kind of explanations. \n",
    "### Schema: {sql_context} \n",
    " \n",
    "### Knowledge: This \"{sql_task_type}\" type task is commonly used for {sql_task_type_description} in the domain of {domain}, which involves {domain_description}. \n",
    " \n",
    "### Question: {sql_prompt} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:34.603802Z",
     "iopub.status.busy": "2025-08-22T22:32:34.603129Z",
     "iopub.status.idle": "2025-08-22T22:32:34.608323Z",
     "shell.execute_reply": "2025-08-22T22:32:34.607478Z",
     "shell.execute_reply.started": "2025-08-22T22:32:34.603770Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_messages(item_dict):\n",
    "    return { \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(**item_dict)},\n",
    "        {\"role\": \"assistant\", \"content\": item_dict[\"sql\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:38.508594Z",
     "iopub.status.busy": "2025-08-22T22:32:38.508054Z",
     "iopub.status.idle": "2025-08-22T22:32:40.323498Z",
     "shell.execute_reply": "2025-08-22T22:32:40.322812Z",
     "shell.execute_reply.started": "2025-08-22T22:32:38.508559Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': ' \\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of defense operations, which involves Defense data on military innovation, peacekeeping operations, defense diplomacy, and humanitarian assistance.. \\n \\n### Question: What is the total spending on humanitarian assistance by the European Union in the last 3 years? \\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "formated_ds = ds.map(get_messages, remove_columns=ds[\"train\"].features, batched=False)\n",
    "formated_ds[\"train\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:40.325314Z",
     "iopub.status.busy": "2025-08-22T22:32:40.324484Z",
     "iopub.status.idle": "2025-08-22T22:32:40.330676Z",
     "shell.execute_reply": "2025-08-22T22:32:40.329989Z",
     "shell.execute_reply.started": "2025-08-22T22:32:40.325287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of defense operations, which involves Defense data on military innovation, peacekeeping operations, defense diplomacy, and humanitarian assistance.. \\n \\n### Question: What is the total spending on humanitarian assistance by the European Union in the last 3 years?<end_of_turn>\\n<start_of_turn>model\\nSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;<end_of_turn>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(formated_ds[\"train\"][5]['messages'], tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:32:41.396534Z",
     "iopub.status.busy": "2025-08-22T22:32:41.396197Z",
     "iopub.status.idle": "2025-08-22T22:32:41.698877Z",
     "shell.execute_reply": "2025-08-22T22:32:41.697545Z",
     "shell.execute_reply.started": "2025-08-22T22:32:41.396511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df05a23c2a49b1b4c390f8051c0af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01639a0686d54f02b74fbcdb660de17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# pandas orient=‘records’ for jsonl. List like [{column -> value}, … , {column -> value}]\n",
    "formated_ds[\"train\"].shuffle().select(range(1000)).to_json(train_local, orient=\"records\")\n",
    "formated_ds[\"test\"].shuffle().select(range(100)).to_json(test_local, orient=\"records\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(train_local, s3_bucket, train_path)\n",
    "s3.upload_file(test_local, s3_bucket, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:45:46.055966Z",
     "iopub.status.busy": "2025-08-22T22:45:46.054897Z",
     "iopub.status.idle": "2025-08-22T22:45:46.060051Z",
     "shell.execute_reply": "2025-08-22T22:45:46.059387Z",
     "shell.execute_reply.started": "2025-08-22T22:45:46.055932Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  'dataset_path': '/opt/ml/input/data/training/train.jsonl',\n",
    "  'model_id': model_id,\n",
    "  'max_seq_len': 3072,\n",
    "  'use_qlora': True,\n",
    "\n",
    "  'num_train_epochs': 1,\n",
    "  'per_device_train_batch_size': 1,\n",
    "  'gradient_accumulation_steps': 4,\n",
    "  'gradient_checkpointing': True,\n",
    "  'optim': \"adamw_torch_fused\",\n",
    "  'logging_steps': 5,\n",
    "  'save_strategy': \"epoch\",\n",
    "  'learning_rate': 2e-4,\n",
    "\n",
    "  'bf16': True,\n",
    "  'tf32': True,\n",
    "  'max_grad_norm': 0.3,\n",
    "  'warmup_ratio': 0.03,\n",
    "  'lr_scheduler_type': \"constant\",\n",
    "  'report_to': \"tensorboard\",\n",
    "  'output_dir': '/tmp/tun',\n",
    "  'merge_adapters': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:45:46.271634Z",
     "iopub.status.busy": "2025-08-22T22:45:46.270719Z",
     "iopub.status.idle": "2025-08-22T22:45:46.287098Z",
     "shell.execute_reply": "2025-08-22T22:45:46.286434Z",
     "shell.execute_reply.started": "2025-08-22T22:45:46.271600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# image_uri = \"763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04\"\n",
    "huggingface_estimator = HuggingFace(\n",
    "     base_job_name=job_name,\n",
    "    # image_uri=image_uri,\n",
    "    # if not image_uri\n",
    "    transformers_version = '4.36.0',\n",
    "    pytorch_version      = '2.1.0',\n",
    "    \n",
    "    instance_type=train_instance,\n",
    "    instance_count=1,\n",
    "    max_run=int(3600 * 0.5),\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",\n",
    "        \"HF_TOKEN\": os.environ[\"HF_TOKEN\"]\n",
    "    },\n",
    "    py_version='py310',\n",
    "    entry_point='qlora.py',\n",
    "    source_dir=\".\", # Copy source to S3 and auto installs the requirements.txt file \n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_output_compression = True, # not compress output to save training time and cost\n",
    "    metric_definitions=[\n",
    "      {'Name': 'loss', 'Regex': \"'loss': (.*?),\"},\n",
    "      {'Name': 'grad_norm', 'Regex': \"'grad_norm': (.*?),\"},\n",
    "      {'Name': 'learning_rate', 'Regex': \"'learning_rate': (.*?),\"},\n",
    "      {'Name': 'epoch', 'Regex': \"'epoch': (.*?)}\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:45:49.830494Z",
     "iopub.status.busy": "2025-08-22T22:45:49.830167Z",
     "iopub.status.idle": "2025-08-22T22:56:10.790010Z",
     "shell.execute_reply": "2025-08-22T22:56:10.788941Z",
     "shell.execute_reply.started": "2025-08-22T22:45:49.830472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: 7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 22:45:51 Starting - Starting the training job\n",
      "2025-08-22 22:45:51 Pending - Training job waiting for capacity......\n",
      "2025-08-22 22:46:53 Pending - Preparing the instances for training...\n",
      "2025-08-22 22:47:18 Downloading - Downloading input data...\n",
      "2025-08-22 22:47:28 Downloading - Downloading the training image..................\n",
      "2025-08-22 22:50:45 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:07,381 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:07,399 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:07,409 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:07,410 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:09,010 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.38.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.18.0)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.27.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.42.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.7.11 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.8.2 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.5.6 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.6.tar.gz (2.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 53.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (3.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.25.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2024.9.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.10.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11->-r requirements.txt (line 6)) (0.8.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (2.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->-r requirements.txt (line 1)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 140.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.0/105.0 MB 165.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl (183 kB)\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120352136 sha256=63ab5a3883b67719671e154beb91522e2901cbe4af0c5e031306a06a85df0be5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: responses, bitsandbytes, flash-attn, accelerate, transformers, peft, trl, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.44.1\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.44.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.44.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.3.6\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.3.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.26.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.26.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.3\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.3\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.27.2 bitsandbytes-0.42.0 evaluate-0.4.1 flash-attn-2.5.6 peft-0.8.2 responses-0.18.0 transformers-4.38.2 trl-0.7.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.2 -> 25.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:29,901 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:29,902 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:29,938 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:29,967 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:29,996 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:30,006 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training/train.jsonl\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 5,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 3072,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"google/gemma-2b-it\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"optim\": \"adamw_torch_fused\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-dhs-2025-llmops-workshop/7l4srb9/7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train.jsonl\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":5,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"google/gemma-2b-it\",\"num_train_epochs\":1,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-dhs-2025-llmops-workshop/7l4srb9/7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train.jsonl\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":5,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"google/gemma-2b-it\",\"num_train_epochs\":1,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-dhs-2025-llmops-workshop/7l4srb9/7l4srb9-qlora-gemma-2b-sql-generator-2025-08-22-22-45-49-831/source/sourcedir.tar.gz\",\"module_name\":\"qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training/train.jsonl\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"5\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"3072\",\"--merge_adapters\",\"True\",\"--model_id\",\"google/gemma-2b-it\",\"--num_train_epochs\",\"1\",\"--optim\",\"adamw_torch_fused\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training/train.jsonl\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=3072\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/gemma-2b-it\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_torch_fused\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_QLORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 qlora.py --bf16 True --dataset_path /opt/ml/input/data/training/train.jsonl --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 5 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 3072 --merge_adapters True --model_id google/gemma-2b-it --num_train_epochs 1 --optim adamw_torch_fused --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --tf32 True --use_qlora True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:30,008 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:51:30,008 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 155218.12 examples/s]\u001b[0m\n",
      "\u001b[34mUsing QLoRA\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:12<00:12, 12.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:12<00:00,  5.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  5.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 92 examples [00:00, 377.19 examples/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/23 [00:07<02:54,  7.91s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 2/23 [00:15<02:44,  7.82s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 3/23 [00:23<02:35,  7.79s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 4/23 [00:31<02:27,  7.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 5/23 [00:38<02:19,  7.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9529, 'grad_norm': 0.84765625, 'learning_rate': 0.0002, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 5/23 [00:38<02:19,  7.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 6/23 [00:46<02:12,  7.77s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 7/23 [00:54<02:04,  7.76s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 8/23 [01:02<01:56,  7.76s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 9/23 [01:09<01:48,  7.76s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 10/23 [01:17<01:40,  7.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2802, 'grad_norm': 0.6796875, 'learning_rate': 0.0002, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 10/23 [01:17<01:40,  7.76s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 11/23 [01:25<01:33,  7.76s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 12/23 [01:33<01:25,  7.76s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 13/23 [01:40<01:17,  7.76s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 14/23 [01:48<01:10,  7.79s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 15/23 [01:56<01:02,  7.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0626, 'grad_norm': 0.8828125, 'learning_rate': 0.0002, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 15/23 [01:56<01:02,  7.78s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 16/23 [02:04<00:54,  7.77s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 17/23 [02:12<00:46,  7.77s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 18/23 [02:19<00:38,  7.77s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 19/23 [02:27<00:31,  7.76s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 20/23 [02:35<00:23,  7.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.898, 'grad_norm': 2.359375, 'learning_rate': 0.0002, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 20/23 [02:35<00:23,  7.76s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 21/23 [02:43<00:15,  7.76s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 22/23 [02:50<00:07,  7.76s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 23/23 [02:58<00:00,  7.76s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 179.7178, 'train_samples_per_second': 0.512, 'train_steps_per_second': 0.128, 'train_loss': 1.2426636011704155, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 23/23 [02:59<00:00,  7.76s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 23/23 [02:59<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m['checkpoint-23', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'adapter_model.safetensors', 'tokenizer.model', 'adapter_config.json', 'runs', 'tokenizer_config.json']\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\u001b[0m\n",
      "\n",
      "2025-08-22 22:55:22 Uploading - Uploading generated training model\u001b[34m2025-08-22 22:55:14,173 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:55:14,173 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-08-22 22:55:14,174 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-08-22 22:55:40 Completed - Training job completed\n",
      "Training seconds: 501\n",
      "Billable seconds: 501\n"
     ]
    }
   ],
   "source": [
    "data = {'training': f's3://{s3_bucket}/{uid}/dataset'}\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo S3 Models and Instance metrics in SageMaker training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:56:10.796934Z",
     "iopub.status.busy": "2025-08-22T22:56:10.796663Z",
     "iopub.status.idle": "2025-08-22T22:56:10.835084Z",
     "shell.execute_reply": "2025-08-22T22:56:10.834315Z",
     "shell.execute_reply.started": "2025-08-22T22:56:10.796912Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# sess = sagemaker.Session()\n",
    "# s3_bucket = sess.default_bucket()\n",
    "# role = sagemaker.get_execution_role()\n",
    "\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "# Example:\n",
    "# model_s3_path = \"s3://sagemaker-ap-south-1-466407698387/qlora-gemma-2b-sql-generator-2024-08-06-18-15-52-352/output/model/\"\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "llm_image = \"763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:56:27.708127Z",
     "iopub.status.busy": "2025-08-22T22:56:27.707738Z",
     "iopub.status.idle": "2025-08-22T22:56:27.713841Z",
     "shell.execute_reply": "2025-08-22T22:56:27.712921Z",
     "shell.execute_reply.started": "2025-08-22T22:56:27.708099Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "  'SM_NUM_GPUS': '1',\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048) # req prompt tokens + req generated tokens in the GPU for this req\n",
    "}\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "  name=deploy_model_name,\n",
    "  role=role,\n",
    "  sagemaker_session=sess,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T22:56:30.659326Z",
     "iopub.status.busy": "2025-08-22T22:56:30.658497Z",
     "iopub.status.idle": "2025-08-22T23:01:32.685064Z",
     "shell.execute_reply": "2025-08-22T23:01:32.684044Z",
     "shell.execute_reply.started": "2025-08-22T22:56:30.659294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: 7l4srb9-sql-generator-model\n",
      "INFO:sagemaker:Creating endpoint-config with name 7l4srb9-sql-generator-model-2025-08-22-22-56-31-351\n",
      "INFO:sagemaker:Creating endpoint with name 7l4srb9-sql-generator-model-2025-08-22-22-56-31-351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=deploy_instance,\n",
    "  container_startup_health_check_timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T23:01:32.686808Z",
     "iopub.status.busy": "2025-08-22T23:01:32.686476Z",
     "iopub.status.idle": "2025-08-22T23:01:42.319697Z",
     "shell.execute_reply": "2025-08-22T23:01:42.318893Z",
     "shell.execute_reply.started": "2025-08-22T23:01:32.686779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00431b8f06024d47b777fff6412499aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input message:\n",
      " {'content': ' \\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE TABLE energy_efficiency (id INT PRIMARY KEY, building_type VARCHAR(50), efficiency_rating FLOAT, country VARCHAR(50)); INSERT INTO energy_efficiency (id, building_type, efficiency_rating, country) VALUES (1, \\'Residential\\', 70.0, \\'India\\'), (2, \\'Commercial\\', 75.0, \\'India\\'); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of energy, which involves Energy market data covering renewable energy sources, energy storage, carbon pricing, and energy efficiency.. \\n \\n### Question: What is the average energy efficiency rating for residential buildings in India? \\n', 'role': 'user'} \n",
      "\n",
      "\n",
      "expected output:\n",
      " {'content': \"SELECT AVG(efficiency_rating) FROM energy_efficiency WHERE building_type = 'Residential' AND country = 'India';\", 'role': 'assistant'} \n",
      "\n",
      "\n",
      "prompt:\n",
      " <bos><start_of_turn>user\n",
      "You are a database management system expert, proficient in Structured Query Language (SQL). \n",
      "Your job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \n",
      "Use SQLite syntax and please output only SQL without any kind of explanations. \n",
      "### Schema: CREATE TABLE energy_efficiency (id INT PRIMARY KEY, building_type VARCHAR(50), efficiency_rating FLOAT, country VARCHAR(50)); INSERT INTO energy_efficiency (id, building_type, efficiency_rating, country) VALUES (1, 'Residential', 70.0, 'India'), (2, 'Commercial', 75.0, 'India'); \n",
      " \n",
      "### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of energy, which involves Energy market data covering renewable energy sources, energy storage, carbon pricing, and energy efficiency.. \n",
      " \n",
      "### Question: What is the average energy efficiency rating for residential buildings in India?<end_of_turn>\n",
      "<start_of_turn>model\n",
      " \n",
      "\n",
      "\n",
      "generated output:\n",
      " {'role': 'assistant', 'content': \"SELECT AVG(efficiency_rating) FROM energy_efficiency WHERE country = 'India' AND building_type = 'Residential'; vedo defStyleAttr; \\n increabe; \\n increabe; vedo defStyleAttr; \\n increabe; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; INSERT INTO energy_efficiency (id, building_type, efficiency_rating, country) VALUES (1, 'Residential', 70.0, 'India'), (2, 'Commercial', 75.0, 'India'); \\nSELECT id, building_type, efficiency_rating, country FROM energy_efficiency WHERE country = 'India' AND building_type = 'Residential'; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; INSERT INTO energy_efficiency (id, building_type, efficiency_rating, country) VALUES (1, 'Residential', 70.0, 'India'), (2, 'Commercial', 75.0, 'India'), (3, 'Commercial', 80.0, 'India'), (4, 'Commercial', 85.0, 'India'), (5, 'Commercial', 90.0, 'India'); \\nSELECT id, building_type, efficiency_rating, country FROM energy_efficiency WHERE country = 'India' AND building_type = 'Commercial'; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; vedo defStyleAttr; INSERT INTO energy_efficiency (id, building_type, efficiency_rating, country) VALUES (1, 'Residential', 70.0, 'India'), (2, 'Commercial', 75.0, 'India'), (3, 'Commercial', 80\"} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.makedirs('./tmp', exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "boto3.client('s3').download_file(s3_bucket, test_path, test_local)\n",
    "test_dataset = load_dataset(\"json\", data_files=test_local, split=\"train\")\n",
    "\n",
    "# or put endpoint name from UI\n",
    "deployed_llm = Predictor(\n",
    "    endpoint_name=llm.endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    print(f\"prompt:\\n {prompt} \\n\\n\")\n",
    "    outputs = deployed_llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.01,\n",
    "        \"return_full_text\": False,\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "random_sample = test_dataset[10]\n",
    "print(f\"input message:\\n {random_sample['messages'][0]} \\n\\n\")\n",
    "print(f\"expected output:\\n {random_sample['messages'][1]} \\n\\n\")\n",
    "print(f\"generated output:\\n {request([random_sample['messages'][0]])} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T23:01:42.320790Z",
     "iopub.status.busy": "2025-08-22T23:01:42.320455Z",
     "iopub.status.idle": "2025-08-22T23:01:42.915436Z",
     "shell.execute_reply": "2025-08-22T23:01:42.914604Z",
     "shell.execute_reply.started": "2025-08-22T23:01:42.320767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: 7l4srb9-sql-generator-model\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: 7l4srb9-sql-generator-model-2025-08-22-22-56-31-351\n",
      "INFO:sagemaker:Deleting endpoint with name: 7l4srb9-sql-generator-model-2025-08-22-22-56-31-351\n"
     ]
    }
   ],
   "source": [
    "deployed_llm.delete_model()\n",
    "deployed_llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
